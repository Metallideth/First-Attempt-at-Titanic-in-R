---
title: "TitanicFirstTry"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r load libraries and data}
library(tidyverse)
library(stringr)
library(ggplot2)
library(GGally)
library(gridExtra)
library(reshape2)
library(lazyeval)
library(mice)
library(VIM)

train <- read.csv("train.csv")
test <-  read.csv("test.csv")
train <- as_tibble(train)
test <- as_tibble(test)

summary(train)
```

Age appears to have NA's

Let's add a feature for title

```{r add Title}
train <- mutate(train, Title = 
                  gsub("^.*, (.*?)\\..*$", "\\1",as.character(Name)))
test <- mutate(test, Title = 
                 gsub("^.*, (.*?)\\..*$", "\\1",as.character(Name)))
train <- select(train, -c(PassengerId,Cabin,Ticket))
test <- select(test, -c(PassengerId,Cabin,Ticket))
levels(train$Embarked)[levels(train$Embarked) == ""] <- "S" #assign the mode
test[is.na(test$Fare),]$Fare = mean(test[!is.na(test$Fare),]$Fare) #assign the mean
```

We'll fix the ages later
In the meantime, let's see how certain variables affect survival

```{r first ggpairs}
ggpairs(data = select(train,-Name))
```

Too many levels in Title.

```{r}
summary(with(train, aov(Survived ~ Title)))
```

Title is significant, clearly

```{r plot Title}
ggplot(data = train, aes(x = Title, y = Survived)) + geom_bar(
    stat = "summary", fun.y = mean) + coord_flip() +
    geom_text(aes(label = ..count.., y= ..prop..), 
    stat= "count", hjust = 1, col = "blue")
```

Some of these could certainly be combined - try Mrs and Miss, Mr and Master

```{r proportion tests}

Mrs <- filter(train, Title %in% c("Mrs"))
Miss <- filter(train, Title %in% c("Miss"))
counts <- c(dim(filter(Mrs))[1],dim(filter(Miss))[1])
lived <- c(dim(filter(Mrs,Survived == 1))[1],dim(filter(Miss,Survived == 1))[1])

Mr <- filter(train, Title %in% c("Mr"))
Master <- filter(train, Title %in% c("Master"))
counts2 <- c(dim(filter(Mr))[1],dim(filter(Master))[1])
lived2 <- c(dim(filter(Mr,Survived == 1))[1],dim(filter(Master,Survived == 1))[1])

prop.test(lived,counts)$p.value
prop.test(lived2,counts2)$p.value

```

I'll assume a strict(er) standard here, 90% significance.

At 90% significance we would reject both null hypotheses that these means are the same, so we should not merge them.

None of the rest of the "Title" values have a sufficient number of values to run statistical tests. Thus, title will merge the following:

Gentlelady <- the Countess, Ms, Mrs, Mme, Mlle, Lady
Miss
Gentleman <- Sir, Master, Major, Dr, Col
Mr <- Rev, Mr, Jonkheer, Don, Capt

In the test set, we'll go with

Gentlelady <- Ms, Mrs
Miss <- Dona, Miss
Gentleman <- Master, Dr, Col
Mr <- Mr, Rev

```{r grouping Titles}
train <- train %>% 
  mutate(Title = replace(Title, Title %in% c("the Countess", "Ms", "Mrs", "Mme", "Mlle", "Lady"),"Gentlelady")) %>% 
  mutate(Title = replace(Title, Title %in% c("Sir", "Master", "Major", "Dr", "Col"),"Gentleman")) %>% 
  mutate(Title = replace(Title, Title %in% c("Rev", "Mr", "Jonkheer", "Don", "Capt"),"Mr"))
  
test <- test %>% 
  mutate(Title = replace(Title, Title %in% c("Ms", "Mrs"),"Gentlelady")) %>% 
  mutate(Title = replace(Title, Title %in% c("Dona", "Miss"),"Miss")) %>% 
  mutate(Title = replace(Title, Title %in% c("Master", "Dr", "Col"),"Gentleman")) %>% 
  mutate(Title = replace(Title, Title %in% c("Mr", "Rev"),"Mr"))

ggplot(data = train, aes(x = Title, y = Survived)) + geom_bar(
    stat = "summary", fun.y = mean) + coord_flip() +
    geom_text(aes(label = ..count.., y= ..prop..), 
    stat= "count", hjust = 1, col = "blue")

```

That's more like it. Let's check Miss vs. Gentlelady just to be sure...

```{r Miss vs. Gentlelady}
Gentlelady <- filter(train, Title %in% c("Gentlelady"))
Miss <- filter(train, Title %in% c("Miss"))
counts <- c(dim(filter(Gentlelady))[1],dim(filter(Miss))[1])
lived <- c(dim(filter(Gentlelady,Survived == 1))[1],dim(filter(Miss,Survived == 1))[1])

prop.test(lived,counts)$p.value
```

P-value is lower than before, indicating that the probabilities of survival for these groups is even more divergent.

Now we need to fill in Age. First, I'll look at the distribution of age and see if the NAs appear to be drawn from the same population using inferential statistics.

```{r complete Age field}
summary(train)
summary(test)

unadjAge <- ggplot(data = subset(train,!is.na(Age)), aes(x = Age)) + geom_histogram(binwidth = 2)

```

Age is skewed right, so we should probably transform it to be normal so it has better regression properties.

```{r Age field transformed}
logAge <- ggplot(data = subset(train,!is.na(Age)), aes(x = log(Age))) + geom_histogram(binwidth = 1/10)
sqrtAge <- ggplot(data = subset(train,!is.na(Age)), aes(x = sqrt(Age))) + geom_histogram(binwidth = 1/5)
otherpowerAge <- ggplot(data = subset(train,!is.na(Age)), aes(x = Age^0.2)) + geom_histogram(binwidth = 1/20)

grid.arrange(unadjAge,logAge,sqrtAge,otherpowerAge,nrow = 1)
```

There's still a great deal of skew to square root. As shown below, a slightly higher power is warranted

```{r Age frequency polygon}
trainagesubset <- subset(train,!is.na(Age))
numpowers <- length(seq(0.1,1,by=0.1))
exponent <- rep(seq(0.1,1,by=0.1),each = dim(trainagesubset)[1])
agerep <- rep(trainagesubset$Age,times = numpowers)
transformage <- data.frame(exponent = as.factor(exponent),transage = agerep^exponent)

ggplot(data = transformage, aes(x = transage)) + geom_freqpoly(aes(color = exponent),bins = 100) +
  coord_cartesian(xlim = c(0,40))

ggplot(data = transformage, aes(x = transage)) + geom_histogram(aes(fill = exponent,color = exponent),bins = 100) + coord_cartesian(xlim = c(0,40))

#ggplot(data = transformage, aes(x = transage)) + geom_area(aes(y = ..count..,fill = exponent), stat = "bin", color = "black", alpha = 0.3, bins = 50)
```

The best exponent appears to be somewhere close to 0.7. To get more specific, let's run a Shapiro-Wilk test on a range of values close by.

```{r}
params <- rep(seq(0.6,0.9,by=0.01),each = dim(trainagesubset)[1])
params <- data.frame(matrix(params,nrow = dim(trainagesubset)[1]))
shapiroparams <- trainagesubset$Age ^ params
shapiroresults <- apply(shapiroparams,2,function(x) shapiro.test(x)$p.value)
maxindex <- which.max(shapiroresults)
exponent <- seq(0.6,0.9,by=0.01)[maxindex]

exponent
```

Looks like an exponent of `r exponent` is returned. Let's check and see what that looks like.

```{r}
ideal <- ggplot(data = train,aes(x = Age ^ exponent)) + geom_histogram()
less <- ggplot(data = train,aes(x = Age ^ 0.5)) + geom_histogram()
more <- ggplot(data = train,aes(x = Age)) + geom_histogram()

grid.arrange(ideal,less,more, nrow = 1)
```

Not very normal, which makes sense considering p-value of `r shapiroresults[maxindex]`. We'll make do with leaving age untransformed.

```{r}
AgeNA <- filter(train, is.na(Age))
AgeNotNA <- filter(train, !is.na(Age))
counts4 <- c(dim(filter(AgeNA))[1],dim(filter(AgeNotNA))[1])
lived4 <- c(dim(filter(AgeNA,Survived == 1))[1],dim(filter(AgeNotNA,Survived == 1))[1])

prop.test(lived4,counts4)

```

It looks like on a raw basis the ratios are different. It's hard to tell if this means that we can't impute Age, considering other variables might be different. We should go ahead and investigate this using pairwise analysis. We'll use the ggpairs visualization tool and explore relationships that appear significant.

```{r ggpairs again}
trainmod <- train
trainmod$Survived <- as.factor(trainmod$Survived)
trainmod$Pclass <- as.factor(trainmod$Pclass)
trainmod <- select(trainmod,-Name)

ggpairs(data = trainmod,upper = list(continuous = "cor",discrete = "facetbar",combo = "box_no_facet"),lower = 
          list(combo = "facethist",discrete = "facetbar",continuous = "points"),axisLabels = "internal")

```

From this, I can see a lot. Survival is much lower among Pclass 3, and slightly higher in Pclass 1. The age of those who survived skews younger. Males died with very high frequency, and females had a higher frequency of survival. SibSp doesn't seem to have a dramatic effect, whereas Parch does. Fare skews higher among the survivors, and embarkation port appears to significantly affect survival.

It is clear upon further reflection that a lot of these independent variables are correlated. Considering the small dataset and number of features, I think it's unlikely that there will be overfitting here, but it's something to look out for during cross-validation.

Specifically for Age, I'm going to do some investigations of flagging those that are coming through as NA and those that arent as separate. I'll investigate the effect other variables have on the survival probabilities of the Age = NA folks, to see if Survival is still significantly even after accounting for various interactions.

```{r}
train <- mutate(train,ageIsNA = is.na(Age))
trainmod <- train
trainmod <- select(trainmod,-c(Name,Age))

{
# summarizer <- function(data, varinquestion, facetvar,dfname,viqval,fvval) {
#     require("dplyr")
#     require("lazyeval")
#     filter_criteria1 <- interp(~data$y == x & data$z == t, .values=list(y = as.name(facetvar), x = fvval, z = as.name(varinquestion), t = viqval,data = as.name(dfname)))
#     filter_criteria2 <- interp(~data$y == x, .values=list(y = as.name(varinquestion),x=viqval,data = as.name(dfname)))
#     varinquestion <- as.name(varinquestion)
#     facetvar <- as.name(facetvar)
#     resultset1 <- data %>% 
#       filter_(filter_criteria1) %>% 
#       summarize(count1 = n())
#     resultset2 <- data %>% 
#       filter_(filter_criteria2) %>% 
#       summarize(count2 = n())
#     return(resultset1$count1/resultset2$count2)
# }
# 
# tablemaker <- function(data, varinquestion, facetvar) {
#   result <- data %>% group_by_(as.name(varinquestion),as.name(facetvar)) %>% summarize_(result = summarizer(data,varinquestion,facetvar,deparse(substitute(data)),as.name(varinquestion),as.name(facetvar)))
#   return(result)
# }
}

Pcvagedata <- trainmod %>% group_by(ageIsNA, Pclass) %>% summarize(Proportion = sum(trainmod$ageIsNA == ageIsNA & trainmod$Pclass == Pclass)/sum(trainmod$ageIsNA == ageIsNA), Count = n())

Pcvage <- ggplot(data = Pcvagedata, aes(x = Pclass, y = Proportion,fill = ageIsNA)) + geom_col(position = "dodge") + geom_text(aes(label = Count), 
     vjust = -0.2, col = "blue",position = position_dodge(width = 0.9)) + coord_cartesian(ylim = c(0,1))

Sexvagedata <- trainmod %>% group_by(ageIsNA, Sex) %>% summarize(Proportion = sum(trainmod$ageIsNA == ageIsNA & trainmod$Sex == Sex)/sum(trainmod$ageIsNA == ageIsNA), Count = n())

Sexvage <- ggplot(data = Sexvagedata, aes(x = Sex, y = Proportion,fill = ageIsNA)) + geom_col(position = "dodge") + geom_text(aes(label = Count), 
     vjust = -0.2, col = "blue",position = position_dodge(width = 0.9)) + coord_cartesian(ylim = c(0,1))

```

At this rate, why not just do the chi squared goodness of fit test to determine if the ageIsNA grouping results in statistically significant groupings of the rest of the elements.

We'll look into the categorical variables first. That includes Pclass, Sex, Embarked, and Title.

```{r}
categoricalsummary <- trainmod %>% group_by(ageIsNA,Pclass,Sex,Embarked,Title) %>% 
  summarize(Count = n())

sexexplore <- categoricalsummary %>% group_by(ageIsNA,Sex) %>% 
  summarize(Count = sum(Count))

expectedprop <- filter(sexexplore,ageIsNA == FALSE) %>% ungroup() %>%
  select(Count)
observed <- filter(sexexplore,ageIsNA == TRUE) %>% ungroup() %>%
  select(Count)
expected <- as.matrix(expectedprop/sum(expectedprop))
observed <- as.matrix(observed)

chisq.test(x = observed,p = expected)

```

At a 95% standard, we accept the hypothesis that the Sex of the ageIsNA and the non-NA groups is distributed equally. Thus, I can use Sex to fill out Age whilst introducing minimal bias into the sample.

```{r}
Pclassexplore <- categoricalsummary %>% group_by(ageIsNA,Pclass) %>% 
  summarize(Count = sum(Count))

expectedprop <- filter(Pclassexplore,ageIsNA == FALSE) %>% ungroup() %>%
  select(Count)
observed <- filter(Pclassexplore,ageIsNA == TRUE) %>% ungroup() %>%
  select(Count)
expected <- as.matrix(expectedprop/sum(expectedprop))
observed <- as.matrix(observed)

chisq.test(x = observed,p = expected)
```

Not so much the case for Pclass. Thus, I'll avoid using this variable to impute Age.

```{r}
Embarkedexplore <- categoricalsummary %>% group_by(ageIsNA,Embarked) %>% 
  summarize(Count = sum(Count))

expectedprop <- filter(Embarkedexplore,ageIsNA == FALSE) %>% ungroup() %>%
  select(Count)
observed <- filter(Embarkedexplore,ageIsNA == TRUE) %>% ungroup() %>%
  select(Count)
expected <- as.matrix(expectedprop/sum(expectedprop))
observed <- as.matrix(observed)

chisq.test(x = observed,p = expected)

Titleexplore <- categoricalsummary %>% group_by(ageIsNA,Title) %>% 
  summarize(Count = sum(Count))

expectedprop <- filter(Titleexplore,ageIsNA == FALSE) %>% ungroup() %>%
  select(Count)
observed <- filter(Titleexplore,ageIsNA == TRUE) %>% ungroup() %>%
  select(Count)
expected <- as.matrix(expectedprop/sum(expectedprop))
observed <- as.matrix(observed)

chisq.test(x = observed,p = expected)
```

Embarked and Title are also determined to be significantly different. Among the categorical variables, only Sex can be used as a valid imputation factor for Age.

Due to the sparse nature of the data SibSp and Parch, I'll convert these into looking at values of 0 or all else, and treat them as a categorical variable.

```{r}
trainmod <- mutate(trainmod, Siblingless = SibSp == 0,
                   Parentless = Parch == 0)

categoricalsummary <- trainmod %>% group_by(ageIsNA,Siblingless,Parentless) %>% 
  summarize(Count = n())

Parchexplore <- categoricalsummary %>% group_by(ageIsNA,Parentless) %>% 
  summarize(Count = sum(Count))

expectedprop <- filter(Parchexplore,ageIsNA == FALSE) %>% ungroup() %>%
  select(Count)
observed <- filter(Parchexplore,ageIsNA == TRUE) %>% ungroup() %>%
  select(Count)
expected <- as.matrix(expectedprop/sum(expectedprop))
observed <- as.matrix(observed)

chisq.test(x = observed,p = expected)

SibSpexplore <- categoricalsummary %>% group_by(ageIsNA,Siblingless) %>% 
  summarize(Count = sum(Count))

expectedprop <- filter(SibSpexplore,ageIsNA == FALSE) %>% ungroup() %>%
  select(Count)
observed <- filter(SibSpexplore,ageIsNA == TRUE) %>% ungroup() %>%
  select(Count)
expected <- as.matrix(expectedprop/sum(expectedprop))
observed <- as.matrix(observed)

chisq.test(x = observed,p = expected)

```

Looks like those are significantly different as well. The last remaining variable is Fare.

```{r}
ggplot(data = trainmod, aes(x = Fare)) + geom_density(aes(fill = ageIsNA),alpha = 0.3)
```
These density graphs look very similar

```{r}
ggplot(data = trainmod, aes(x = Fare)) + geom_histogram(aes(fill = ageIsNA)) + facet_wrap(~ageIsNA,nrow = 2,scales = "free")

filter(trainmod,ageIsNA == TRUE)$Fare %>% summary()
filter(trainmod,ageIsNA == FALSE)$Fare %>% summary()
```

However upon looking closer at the statistics it appears that these are in fact distributed very differently, with vastly different means and medians. They are so different that a statistical test isn't really needed here. At long last, we can impute the Age values solely based on Sex.

```{r}
set.seed(2000)
trainmod <- select(train,c(Sex,Age))
impute1 <- mice(trainmod, m =5)
impute2 <- complete(impute1)
train <- mutate(train,imputeAge = impute2$Age)

testmod <- select(test, c(Sex,Age))
impute1 <- mice(testmod, m = 5)
impute2 <- complete(impute1)
test <- mutate(test,imputeAge = impute2$Age)
```

Now we're ready to start breaking down features. I am planning to use some combination of Pclass, Sex, SibSp, Parch, Fare, Embarked, Title, and imputeAge to model this. I'll leave age and fare continuous for now. Split the training set up into a training and cross validation set.


